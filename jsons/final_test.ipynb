{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import math\n",
    "import numpy\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "# new import\n",
    "import codecs\n",
    "import io\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports to check if file exists\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK stopwords if you haven't already\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    # path = Docs.objects.get(id = n).address;\n",
    "    # above line gives error \"no such table: project_template_docs\"\n",
    "    file = open(path)\n",
    "    transcripts = json.load(file)\n",
    "    return transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(inv_idx, n_docs, min_df=2, max_df_ratio=0.90):\n",
    "    \"\"\" Compute term IDF values from the inverted index.\n",
    "\n",
    "    Words that are too frequent or too infrequent get pruned.\n",
    "\n",
    "    Arguments\n",
    "    =========\n",
    "\n",
    "    inv_idx: an inverted index as above\n",
    "\n",
    "    n_docs: int,\n",
    "        The number of documents.\n",
    "\n",
    "    min_df: int,\n",
    "        Minimum number of documents a term must occur in.\n",
    "        Less frequent words get ignored.\n",
    "\n",
    "    max_df_ratio: float,\n",
    "        Maximum ratio of documents a term can occur in.\n",
    "        More frequent words get ignored.\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "\n",
    "    idf: dict\n",
    "        For each term, the dict contains the idf value.\n",
    "\n",
    "    \"\"\"\n",
    "    idf = {}\n",
    "    for term in inv_idx:\n",
    "        df = len(inv_idx[term])\n",
    "        if df >= min_df and float(df/n_docs) <= max_df_ratio:\n",
    "            idf[term] = math.log(float(n_docs/(1+df)))\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_doc_norms(index, idf, n_docs):\n",
    "    \"\"\" Precompute the euclidean norm of each document.\n",
    "\n",
    "    Arguments\n",
    "    =========\n",
    "\n",
    "    index: the inverted index as above\n",
    "\n",
    "    idf: dict,\n",
    "        Precomputed idf values for the terms.\n",
    "\n",
    "    n_docs: int,\n",
    "        The total number of documents.\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "\n",
    "    norms: np.array, size: n_docs\n",
    "        norms[i] = the norm of document i.\n",
    "    \"\"\"\n",
    "    norms = [0 for _ in range(n_docs)]\n",
    "    for term in index:\n",
    "        for i, tf in index[term]:\n",
    "            norms[i] += math.pow((tf*idf[term]), 2)\n",
    "\n",
    "    norm_array = numpy.array(norms)\n",
    "    return numpy.sqrt(norm_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_set(context):\n",
    "    \"\"\" Computes the set of all words used in a list of strings.\n",
    "\n",
    "    Arguments\n",
    "    =========\n",
    "\n",
    "    context: a list of strings\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "\n",
    "    word_set: set of distinct words\n",
    "    \"\"\"\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    sw = stopwords.words('english')\n",
    "    word_list = []\n",
    "    for string in context:\n",
    "        tkns = tokenizer.tokenize(string)\n",
    "        for tk in tkns:\n",
    "            if tk not in sw:\n",
    "                word_list.append(tk)\n",
    "    word_set = set(word_list)\n",
    "    return word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_strip(s):\n",
    "    \"\"\"Strips punctuation from a string and returns the lowercase version. Doesn't work for crazy punctuation\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "\n",
    "    s : a string\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "\n",
    "    new_quote : a punctuation stripped string\n",
    "    \"\"\"\n",
    "    punctuation = (\".\", \"/\", \":\", \",\", \";\", \"!\", \"?\", \"(\", \")\", '\"', \"-\", \"]\", \"'\", \"[\", \"#\", \"$\")\n",
    "    l = s.lower().split()\n",
    "    new_quote = \"\"\n",
    "    for x in l:\n",
    "        if (x == \"-\"):\n",
    "            continue\n",
    "        else:\n",
    "            new_x = x.strip()\n",
    "            if (new_x.endswith(\"...\")):\n",
    "                new_x = new_x[:len(x)-3]\n",
    "            if new_x.endswith(punctuation):\n",
    "                new_x = new_x[:len(x)-1]\n",
    "            newer_x = new_x\n",
    "            if newer_x.startswith(punctuation):\n",
    "                newer_x = newer_x[1:]\n",
    "            new_quote = new_quote + newer_x +  \" \"\n",
    "    return new_quote.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune quotes\n",
    "def quote_pruner(context, quotes, movies):\n",
    "    \"\"\"We don't want useless or meaningless quotes so we prune them out.\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "\n",
    "    context: a list of strings\n",
    "    \n",
    "    quotes : a list of strings\n",
    "    \n",
    "    movies : a list of strings\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "\n",
    "    context, quotes, movies : a pruned context, movies, and quotes, based on quote pruning\n",
    "    \"\"\"\n",
    "    kill_sign = \"asdf no good this quote is toodleloos\"\n",
    "    single_list = (\".\", \"/\", \":\", \",\", \";\", \"!\", \"?\", \"(\", \")\", '\"', \"-\", \"]\", \"'\", \"[\", \"#\", \"$\", \n",
    "                  \"well\", \"what\", \"why\", \"goodnight\", \"hello\", \"how\", \"no\", \"yes\", \"yep\", \"nope\", \"pick\", \"cigarette\", \n",
    "                  \"obviously\", \"kay\", \"scrappy\", \"good\", \"bad\", \"great\", \"goodbye\", \"who\", \"hoke\", \"yes'm\", \"come\",\n",
    "                  \"ouch\", \"huh\", \"shit\", \"ed\", \"fuck\", \"oh\", \"right\", \"ebay\", \"nothing\", \"me\", \"you\", \"mount\", \"pray\", \"sometimes\",\n",
    "                  \"really\", \"ditto\", \"jeez\", \"exactly\", \"bull\", \"bullshit\", \"yep\", \"bing\", \"38\", \"occupation\", \"pyscho\", \"ok\", \n",
    "                  \"okay\", \"ooh\", \"dance\", \"terrific\", \"cuban\", \"mexican\", \"but\", \"blue\", \"wood\", \"apples\", \"cider\", \"exactly\",\n",
    "                  \"david\", \"fire\", \"y'all\", \"so\", \"always\", \"hey\")\n",
    "    \n",
    "    index_list = []\n",
    "    \n",
    "    for x in range(len(quotes)):\n",
    "        y = quotes[x].split()\n",
    "        if len(y) == 1:\n",
    "            no_punct = punct_strip(y[0])\n",
    "            if (no_punct in single_list) or (len(no_punct) == 0):\n",
    "                context[x] = kill_sign\n",
    "                quotes[x] = kill_sign\n",
    "                movies[x] = kill_sign\n",
    "    \n",
    "    # Remove all kill signs\n",
    "    while (kill_sign in movies):\n",
    "        context.remove(kill_sign)\n",
    "        quotes.remove(kill_sign)\n",
    "        movies.remove(kill_sign)\n",
    "        \n",
    "    return context, quotes, movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_words(inv_ind):\n",
    "    \"\"\"\n",
    "    Returns the set of all non-stop-word in the inverted index\n",
    "    \"\"\"\n",
    "    # Get English stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # Find set of words that aren't stop words and are in our tfidf\n",
    "    base_words = []\n",
    "    word_to_index = {}\n",
    "    for word in inv_ind:\n",
    "        if word not in stop_words:\n",
    "            base_words.append(word)\n",
    "    for i, word in enumerate(base_words):\n",
    "        word_to_index[word] = i\n",
    "    \n",
    "    # Return word list and mapping\n",
    "    return word_list, word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_word_counts(lst, word):\n",
    "    \"\"\"Update the list for a word co-occurrance matrix\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "\n",
    "    lst: the pair (word,count) list from the word occurrance\n",
    "    \n",
    "    word : the word we're updating\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "\n",
    "    new_list : an updated word occurrance list\n",
    "    \"\"\"\n",
    "    found = False\n",
    "    new_list = []\n",
    "    for w,c in lst:\n",
    "        if word == w:\n",
    "            found = True\n",
    "            count = c+1\n",
    "            new_list.append((w,count))\n",
    "            break\n",
    "        else:\n",
    "            new_list.append((w,c))\n",
    "    if not(found):\n",
    "        new_list.append((word,1))\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_basic_cooccurance(word_list):\n",
    "    \"\"\" Initialize the base word co-occurrance list from our context and quotes.\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "\n",
    "    word_list: the list of words which are in our movie space\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "\n",
    "    word_co : a dictionary representing the word_occurrance matrix\n",
    "    \"\"\"\n",
    "    # Get English stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # Merge context and quotes\n",
    "    quote_list = f_quotes\n",
    "    new_quote_list = []\n",
    "    for q in quote_list:\n",
    "        new_q = punct_strip(q)\n",
    "        if new_q not in f_context:\n",
    "            new_quote_list.append(new_q)\n",
    "    context_quotes = f_context + new_quote_list\n",
    "    \n",
    "    # Find co occurances in context data, based co-occurances in a document\n",
    "    word_co = defaultdict(list)\n",
    "    word_count_dict = defaultdict(int)\n",
    "    for doc in context_quotes:\n",
    "        # Double loop to count word co-occurances\n",
    "        tkns = f_tokenizer.tokenize(doc)\n",
    "        for i in range(len(tkns)):\n",
    "            if tkns[i] not in stop_words:\n",
    "                word_count_dict[tkns[i]] += 1\n",
    "                for j in range(len(tkns)):\n",
    "                    if not(j == i) and (tkns[j] in word_list):\n",
    "                        word_co[tkns[i]] = update_word_counts(word_co[tkns[i]], tkns[j])\n",
    "    return word_co, word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_cooccurance(word_co, word_count_dict, word_list, docs):\n",
    "    \"\"\" Updates the word co-occurrance mat and the word count dict with a new set of data.\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "\n",
    "    word_co: a word co-occurrance matrix in the form of a dictionary\n",
    "    \n",
    "    word_count_dict: a dictionary that keeps track of the total occurances of a word\n",
    "    \n",
    "    word_list: the list of words which are in our movie space\n",
    "    \n",
    "    docs: the new docs we're using to update our word co-occurance\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "    \n",
    "    word_co, word_count_dict : new word co-occurance dict/mat and new word count dictionary\n",
    "    \"\"\"\n",
    "    # Get English stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    word_co = defaultdict(list)\n",
    "    # Find co occurances in context data, based on document (content)\n",
    "    for doc in docs:\n",
    "        # Double loop to count word co-occurances\n",
    "        tkns = f_tokenizer.tokenize(punct_strip(doc))\n",
    "        for i in range(len(tkns)):\n",
    "            if tkns[i] not in stop_words:\n",
    "                for j in range(len(tkns)):\n",
    "                    if not(j == i) and (tkns[j] in word_list):\n",
    "                        word_co[tkns[i]] = update_word_co(word_co[tkns[i]], tkns[j])\n",
    "    return word_co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pmi(word_co, word_count_dict):\n",
    "    \"\"\" Calculate the pmi of a word based on the word co-occurances\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "\n",
    "    word_co: a word occurrance matrix in the form of a dictionary\n",
    "    \n",
    "    word_list: the list of words which are in our movie space\n",
    "    \n",
    "    docs: the new docs we're using to update our word co-occurance\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "    pmi_dict = a dictionary like word_co but has the pmi's instead\n",
    "    \"\"\"\n",
    "    # PMI(x,y) = log[p(x,y)/(p(x)*p(y))], assuming p(x,y) = co-occurances over total\n",
    "    pmi_dict = defaultdict(list)\n",
    "    \n",
    "    # Find total\n",
    "    total = 0\n",
    "    for key in word_count_dict:\n",
    "        total += word_count_dict[key]\n",
    "    total += 1 # smoothing\n",
    "    \n",
    "    # Caclulate PMIs\n",
    "    for key in word_co:\n",
    "        p_x = (word_count_dict[key] + 0.0) / total\n",
    "        pmi_list = []\n",
    "        lst = word_co[key]\n",
    "        for word,co in lst:\n",
    "            p_y = (word_count_dict[word] + 0.0) / total\n",
    "            p_x_y = (co + 0.0) / (word_count_dict[key] + 1.0 + word_count_dict[word])\n",
    "            res = math.log(p_x_y / (p_x * p_y + 1.0))\n",
    "            pmi_list.append((word,res))\n",
    "        pmi_dict[key] = pmi_list\n",
    "    \n",
    "    return pmi_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------#\n",
    "#--------------------------Final Version Init------------------------------------#\n",
    "#--------------------------------------------------------------------------------#\n",
    "context_file = \"final_context_1.json\"\n",
    "movie_file = \"final_movies_1.json\"\n",
    "quote_file = \"final_quotes_1.json\"\n",
    "year_rating_file = \"final_year_rating_1.json\"\n",
    "\n",
    "f_context = read_file(context_file)\n",
    "f_movies = read_file(movie_file)\n",
    "f_quotes = read_file(quote_file)\n",
    "f_year_rating_dict = read_file(year_rating_file)\n",
    "\n",
    "# Reincode to unicode\n",
    "for i in range(len(f_context)):\n",
    "    f_context[i] = f_context[i].encode(\"utf-8\").decode(\"utf-8\")\n",
    "    f_movies[i] = f_movies[i].encode(\"utf-8\").decode(\"utf-8\")\n",
    "    f_quotes[i] = f_quotes[i].encode(\"utf-8\").decode(\"utf-8\")\n",
    "\n",
    "f_context, f_quotes, f_movies = quote_pruner(f_context, f_quotes, f_movies)\n",
    "    \n",
    "# Initialize query tokenizer\n",
    "f_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# Create inverted index mapping terms to quotes\n",
    "# f_inverted_index = {}\n",
    "# for context_id, context in enumerate(f_context):\n",
    "#     context_tokens = f_tokenizer.tokenize(context)\n",
    "#     for term in context_tokens:\n",
    "#         if term not in stop_words:\n",
    "#             if term in f_inverted_index:\n",
    "#                 lst = f_inverted_index[term]\n",
    "#                 found = False\n",
    "#                 for i, tup in enumerate(lst):\n",
    "#                     if context_id == tup[0]:\n",
    "#                         lst[i] = (context_id, tup[1] + 1)\n",
    "#                         found = True\n",
    "#                 if not found:\n",
    "#                     f_inverted_index[term].append((context_id, 1))\n",
    "#             else:\n",
    "#                 f_inverted_index[term] = [(context_id, 1)]\n",
    "f_inverted_index = {} # Including Stop Words\n",
    "for context_id, context in enumerate(f_context):\n",
    "    context_tokens = f_tokenizer.tokenize(context)\n",
    "    for term in context_tokens:\n",
    "        if term in f_inverted_index:\n",
    "            lst = f_inverted_index[term]\n",
    "            found = False\n",
    "            for i, tup in enumerate(lst):\n",
    "                if context_id == tup[0]:\n",
    "                    lst[i] = (context_id, tup[1] + 1)\n",
    "                    found = True\n",
    "            if not found:\n",
    "                f_inverted_index[term].append((context_id, 1))\n",
    "        else:\n",
    "            f_inverted_index[term] = [(context_id, 1)]\n",
    "\n",
    "# Compute idf values for each term\n",
    "f_idf = compute_idf(f_inverted_index, len(f_context))\n",
    "# Prune out values removed by idf\n",
    "f_inverted_index = {key: val for key, val in f_inverted_index.items() if key in f_idf}\n",
    "# Compute document norms\n",
    "f_norms = compute_doc_norms(f_inverted_index, f_idf, len(f_context))\n",
    "\n",
    "# Check if there is a word co-occurrance file, word count file, and pmi file. if not, remake\n",
    "word_co_filename = \"word_co.json\"\n",
    "word_count_filename = \"word_count_dict.json\"\n",
    "pmi_dict_filename = \"pmi_dict.json\"\n",
    "if ((os.path.isfile(word_co_filename)) and (os.path.isfile(word_count_filename))) and (os.path.isfile(pmi_dict_filename)):\n",
    "    # Read files\n",
    "    word_co = read_file(word_co_filename)\n",
    "    word_count_dict = read_file(word_count_filename)\n",
    "    pmi_dict = read_file(pmi_dict_filename)\n",
    "else:\n",
    "    # Initialize word co-occurance matrix (merge this with above cell)\n",
    "    # This may take a while\n",
    "    word_list, word_to_index = get_context_words(f_inverted_index)\n",
    "    word_co, word_count_dict = find_basic_cooccurance(word_list)\n",
    "    # Get PMI\n",
    "    pmi_dict = find_pmi(word_co, word_count_dict)\n",
    "    # Write data\n",
    "    with io.open(\"word_co.json\",'w',encoding=\"utf-8\") as fout:\n",
    "        fout.write(unicode(json.dumps(word_co, ensure_ascii=False)))\n",
    "    with io.open(\"word_count_dict.json\",'w',encoding=\"utf-8\") as fout:\n",
    "        fout.write(unicode(json.dumps(word_count_dict, ensure_ascii=False)))\n",
    "    with io.open(\"pmi_dict.json\",'w',encoding=\"utf-8\") as fout:\n",
    "        fout.write(unicode(json.dumps(pmi_dict, ensure_ascii=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------NEW METHODS-----------------------------------#\n",
    "def year_rating_weight(year, rating, cosine, cur_year=2016, min_year=1925, year_weight=0.3,\n",
    "                      rating_weight=0.7, cosine_weight=0.8, y_r_weight=0.2):\n",
    "    \n",
    "    \"\"\" Compute new score with weighting from the cosine similarity with\n",
    "    the release year and rating of the movie.\n",
    "\n",
    "    Arguments\n",
    "    =========\n",
    "\n",
    "    year: float, the year of the movie\n",
    "\n",
    "    rating: float, the rating of the movie (out of 10)\n",
    "\n",
    "    cosine: float, the cosine similarity of the query against the movie context\n",
    "\n",
    "    cur_year, min_year: int, current year and minimum year (the lowest year)\n",
    "    \n",
    "    year_weight, rating_weight: the weights for the year and rating\n",
    "    \n",
    "    cosine_weight, y_r_weight: the weights for the cosine sim vs the year and rating value\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "\n",
    "    a new score that has been weighted with the year and rating of the movie \n",
    "    \"\"\"\n",
    "    \n",
    "    w_year = (((cur_year-min_year)-(cur_year-year))/(cur_year-min_year))*year_weight\n",
    "    w_rating = (rating/10.0)*rating_weight\n",
    "    return ((w_year+w_rating)*y_r_weight) + (cosine*cosine_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_vectorize(q, sw=False):\n",
    "    # Remove punctuation, lowercase, and encode to utf\n",
    "    query = punct_strip(q.lower().encode(\"utf-8\").decode(\"utf-8\"))\n",
    "    \n",
    "    # Tokenize query and check query stopword cutoff\n",
    "    query_words = f_tokenizer.tokenize(query)\n",
    "    \n",
    "    # Remove stop words if necessary\n",
    "    stop_words = stopwords.words('english') # Get English stop words\n",
    "    if (sw):\n",
    "        new_query = []\n",
    "        for x in query_words:\n",
    "            if x not in stop_words:\n",
    "                new_query.append(x)\n",
    "        query_words = new_query\n",
    "    \n",
    "    # Make query tfidf\n",
    "    query_tfidf = defaultdict(int)\n",
    "    for word in query_words:\n",
    "        query_tfidf[word] += 1\n",
    "    for word in query_tfidf:\n",
    "        if word in f_idf:\n",
    "            query_tfidf[word] *= f_idf[word]\n",
    "        else:\n",
    "            query_tfidf[word] = 0\n",
    "    \n",
    "    # Find query norm\n",
    "    query_norm = 0\n",
    "    for word in query_tfidf:\n",
    "        query_norm += math.pow(query_tfidf[word], 2)\n",
    "    query_norm = math.sqrt(query_norm)\n",
    "    \n",
    "    return query_tfidf, query_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_rocchio(query, relevant, sw=False, a=.3, b=.4, clip = True):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        query: a string representing the name of the movie being queried for\n",
    "        \n",
    "        relevant: a list of int representing the indices of relevant movies for query\n",
    "        \n",
    "        irrelevant: a list of strings representing the names of irrelevant movies for query\n",
    "        \n",
    "        a,b: floats, corresponding to the weighting of the original query, relevant queriesrespectively.\n",
    "        \n",
    "        clip: boolean, whether or not to clip all returned negative values to 0\n",
    "        \n",
    "    Returns:\n",
    "        q_mod: a dict representing the modified query vector. this vector should have no negatve\n",
    "        weights in it!\n",
    "    \"\"\"\n",
    "    \n",
    "    relevant_id = []\n",
    "    for s,i in relevant:\n",
    "        relevant_id.append(i)\n",
    "    \n",
    "    # vectorize query\n",
    "    query_tfidf, query_norm = query_vectorize(query, sw)\n",
    "    \n",
    "    if query_norm == 0:\n",
    "        return f_find_random()\n",
    "    \n",
    "    # Calculate alpha*query_vec\n",
    "    query_vec = query_tfidf\n",
    "    for word in query_vec:\n",
    "        query_vec[word] /= query_norm\n",
    "        query_vec[word] *= a\n",
    "    \n",
    "    # Get words in relevant docs\n",
    "    relevant_words = []\n",
    "    relevant_context = []\n",
    "    for i in relevant_id:\n",
    "        relevant_context.append(f_context[i])\n",
    "    for context in relevant_context:\n",
    "        context_tkns = f_tokenizer.tokenize(context)\n",
    "        for tkn in context_tkns:\n",
    "            if tkn not in relevant_words:\n",
    "                relevant_words.append(tkn)\n",
    "    \n",
    "    # Collect relevant doc vector sums\n",
    "    relevant_docs = defaultdict(int)\n",
    "    for word in relevant_words:\n",
    "        if word in f_inverted_index:\n",
    "            for quote_id, tf in f_inverted_index[word]:\n",
    "                if quote_id in relevant_id:\n",
    "                    relevant_docs[word] += (tf / f_norms[quote_id])\n",
    "    \n",
    "    # Calculate beta term\n",
    "    beta_term = b * (1.0 / len(relevant))\n",
    "    for key in relevant_docs:\n",
    "        relevant_docs[key] *= beta_term\n",
    "    \n",
    "    # Sum query and relevant\n",
    "    q_mod = {k: query_vec.get(k,0) + relevant_docs.get(k,0.0) for k in set(query_vec) | set(relevant_docs)}\n",
    "\n",
    "    # negative checks for terms, if clip\n",
    "    if (clip):\n",
    "        for key in q_mod:\n",
    "            if q_mod[key] < 0:\n",
    "                q_mod[key] = 0\n",
    "        return q_mod\n",
    "    else:\n",
    "        return q_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_final(q, rocchio=True, psuedo_rocchio_num=6, sw=False, pmi_num=10):\n",
    "    # Vectorize query\n",
    "    query_tfidf, query_norm = query_vectorize(q, sw)\n",
    "    \n",
    "    if query_norm == 0:\n",
    "        return f_queries\n",
    "    \n",
    "    # Expand query using PMI\n",
    "    # Use the top x number of co-occurances to expand the query\n",
    "    \n",
    "    \n",
    "    # Get scores\n",
    "    scores = [0 for _ in f_quotes]\n",
    "    for word in query_tfidf:\n",
    "        if word in f_inverted_index:\n",
    "            for quote_id, tf in f_inverted_index[word]:\n",
    "                scores[quote_id] += query_tfidf[word]*tf*f_idf[word]\n",
    "\n",
    "    results = []\n",
    "    for i, s in enumerate(scores):\n",
    "        if f_norms[i] != 0:\n",
    "            results.append((s/(f_norms[i]*query_norm), i))\n",
    "    \n",
    "    # Weight scores with year and rating\n",
    "    for i in range(len(results)):\n",
    "        score = results[i][0]\n",
    "        index = results[i][1]\n",
    "        year = f_year_rating_dict[f_movies[i]][0]\n",
    "        rating = f_year_rating_dict[f_movies[i]][1]\n",
    "        results[i] = (year_rating_weight(float(year), float(rating), score), index)\n",
    "    \n",
    "    # sort results\n",
    "    results.sort(reverse=True)\n",
    "    \n",
    "    if rocchio:\n",
    "        # Do pseudo-relevance feedback with Rocchio\n",
    "        mod_query = pseudo_rocchio(q, results[:psuedo_rocchio_num], sw)\n",
    "        mod_query_norm = 0\n",
    "        for word in mod_query:\n",
    "            mod_query_norm += math.pow(mod_query[word], 2)\n",
    "        mod_query_norm = math.sqrt(mod_query_norm)\n",
    "\n",
    "\n",
    "        # Re-find scores and reweight with year and rating\n",
    "        scores = [0 for _ in f_quotes]\n",
    "        for word in mod_query:\n",
    "            if word in f_inverted_index:\n",
    "                for quote_id, tf in f_inverted_index[word]:\n",
    "                    scores[quote_id] += mod_query[word]*tf*f_idf[word]\n",
    "\n",
    "        results = []\n",
    "        for i, s in enumerate(scores):\n",
    "            if f_norms[i] != 0:\n",
    "                results.append((s/(f_norms[i]*mod_query_norm), i))\n",
    "\n",
    "        # Weight scores with year and rating\n",
    "        for i in range(len(results)):\n",
    "            score = results[i][0]\n",
    "            index = results[i][1]\n",
    "            year = f_year_rating_dict[f_movies[i]][0]\n",
    "            rating = f_year_rating_dict[f_movies[i]][1]\n",
    "            results[i] = (year_rating_weight(float(year), float(rating), score), index)\n",
    "    \n",
    "    # Sort and return results\n",
    "    top_res_num = 5\n",
    "    results.sort(reverse=True)\n",
    "    result_quotes = [\"{} - {}\".format(f_quotes[i], f_movies[i]) for _, i in results[:top_res_num]]\n",
    "    return result_quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_query(q, rocchio=True):\n",
    "    x = find_final(q, rocchio)\n",
    "    for y in x:\n",
    "        print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_context(q):\n",
    "    for x in range(len(f_context)):\n",
    "        if f_quotes[x] == q:\n",
    "            print(f_context[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diane. - trainspotting\n",
      "Fuck you. That's my name. You know why, mister? You drove a Hyundai to get here. I drove an eighty-thousand dollar BMW. THAT'S my name. And your name is you're wanting. You can't play in the man's game, you can't close them - go home and tell your wife your troubles. Because only one thing counts in this life: Get them to sign on the line which is dotted. You hear me you fucking faggots? A-B-C. A-Always, B-Be, C-Closing. Always be closing. ALWAYS BE CLOSING. A-I-D-A. Attention, Interest, Decision, Action. Attention - Do I have your attention? Interest - Are you interested? I know you are, 'cause it's fuck or walk. You close or you hit the bricks. Decision - Have you made your decision for Christ? And Action. A-I-D-A. Get out there - you got the prospects coming in. You think they came in to get out of the rain? A guy don't walk on the lot lest he wants to buy. They're sitting out there waiting to give you their money. Are you gonna take it? Are you man enough to take it? What's the problem, pal? - Glengarry Glen Ross\n",
      "I'm Beth. - the 40-year-old virgin\n",
      "Sir Robin of Camelot. - Monty Python and the Holy Grail\n",
      "My name is Sir Lancelot of Camelot. - Monty Python and the Holy Grail\n"
     ]
    }
   ],
   "source": [
    "system_query(\"what's your name\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diane. - trainspotting\n",
      "Fuck you. That's my name. You know why, mister? You drove a Hyundai to get here. I drove an eighty-thousand dollar BMW. THAT'S my name. And your name is you're wanting. You can't play in the man's game, you can't close them - go home and tell your wife your troubles. Because only one thing counts in this life: Get them to sign on the line which is dotted. You hear me you fucking faggots? A-B-C. A-Always, B-Be, C-Closing. Always be closing. ALWAYS BE CLOSING. A-I-D-A. Attention, Interest, Decision, Action. Attention - Do I have your attention? Interest - Are you interested? I know you are, 'cause it's fuck or walk. You close or you hit the bricks. Decision - Have you made your decision for Christ? And Action. A-I-D-A. Get out there - you got the prospects coming in. You think they came in to get out of the rain? A guy don't walk on the lot lest he wants to buy. They're sitting out there waiting to give you their money. Are you gonna take it? Are you man enough to take it? What's the problem, pal? - Glengarry Glen Ross\n",
      "I'm Beth. - the 40-year-old virgin\n",
      "Sir Robin of Camelot. - Monty Python and the Holy Grail\n",
      "My name is Sir Lancelot of Camelot. - Monty Python and the Holy Grail\n"
     ]
    }
   ],
   "source": [
    "system_query(\"what's your name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}