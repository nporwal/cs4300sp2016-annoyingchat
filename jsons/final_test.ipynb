{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import math\n",
    "import numpy\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "# new import\n",
    "import codecs\n",
    "import io\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports to check if file exists\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK stopwords if you haven't already\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    # path = Docs.objects.get(id = n).address;\n",
    "    # above line gives error \"no such table: project_template_docs\"\n",
    "    file = open(path)\n",
    "    transcripts = json.load(file)\n",
    "    return transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(inv_idx, n_docs, min_df=2, max_df_ratio=0.90):\n",
    "    \"\"\" Compute term IDF values from the inverted index.\n",
    "\n",
    "    Words that are too frequent or too infrequent get pruned.\n",
    "\n",
    "    Arguments\n",
    "    =========\n",
    "\n",
    "    inv_idx: an inverted index as above\n",
    "\n",
    "    n_docs: int,\n",
    "        The number of documents.\n",
    "\n",
    "    min_df: int,\n",
    "        Minimum number of documents a term must occur in.\n",
    "        Less frequent words get ignored.\n",
    "\n",
    "    max_df_ratio: float,\n",
    "        Maximum ratio of documents a term can occur in.\n",
    "        More frequent words get ignored.\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "\n",
    "    idf: dict\n",
    "        For each term, the dict contains the idf value.\n",
    "\n",
    "    \"\"\"\n",
    "    idf = {}\n",
    "    for term in inv_idx:\n",
    "        df = len(inv_idx[term])\n",
    "        if df >= min_df and float(df/n_docs) <= max_df_ratio:\n",
    "            idf[term] = math.log(float(n_docs/(1+df)))\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_doc_norms(index, idf, n_docs):\n",
    "    \"\"\" Precompute the euclidean norm of each document.\n",
    "\n",
    "    Arguments\n",
    "    =========\n",
    "\n",
    "    index: the inverted index as above\n",
    "\n",
    "    idf: dict,\n",
    "        Precomputed idf values for the terms.\n",
    "\n",
    "    n_docs: int,\n",
    "        The total number of documents.\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "\n",
    "    norms: np.array, size: n_docs\n",
    "        norms[i] = the norm of document i.\n",
    "    \"\"\"\n",
    "    norms = [0 for _ in range(n_docs)]\n",
    "    for term in index:\n",
    "        for i, tf in index[term]:\n",
    "            norms[i] += math.pow((tf*idf[term]), 2)\n",
    "\n",
    "    norm_array = numpy.array(norms)\n",
    "    return numpy.sqrt(norm_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_set(context):\n",
    "    \"\"\" Computes the set of all words used in a list of strings.\n",
    "\n",
    "    Arguments\n",
    "    =========\n",
    "\n",
    "    context: a list of strings\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "\n",
    "    word_set: set of distinct words\n",
    "    \"\"\"\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    sw = stopwords.words('english')\n",
    "    word_list = []\n",
    "    for string in context:\n",
    "        tkns = tokenizer.tokenize(string)\n",
    "        for tk in tkns:\n",
    "            if tk not in sw:\n",
    "                word_list.append(tk)\n",
    "    word_set = set(word_list)\n",
    "    return word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_strip(s):\n",
    "    \"\"\"Strips punctuation from a string and returns the lowercase version. Doesn't work for crazy punctuation\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "\n",
    "    s : a string\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "\n",
    "    new_quote : a punctuation stripped string\n",
    "    \"\"\"\n",
    "    punctuation = (\".\", \"/\", \":\", \",\", \";\", \"!\", \"?\", \"(\", \")\", '\"', \"-\", \"]\", \"'\", \"[\", \"#\", \"$\")\n",
    "    l = s.lower().split()\n",
    "    new_quote = \"\"\n",
    "    for x in l:\n",
    "        if (x == \"-\"):\n",
    "            continue\n",
    "        else:\n",
    "            new_x = x.strip()\n",
    "            if (new_x.endswith(\"...\")):\n",
    "                new_x = new_x[:len(x)-3]\n",
    "            if new_x.endswith(punctuation):\n",
    "                new_x = new_x[:len(x)-1]\n",
    "            newer_x = new_x\n",
    "            if newer_x.startswith(punctuation):\n",
    "                newer_x = newer_x[1:]\n",
    "            new_quote = new_quote + newer_x +  \" \"\n",
    "    return new_quote.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune quotes\n",
    "def quote_pruner(context, quotes, movies):\n",
    "    \"\"\"We don't want useless or meaningless quotes so we prune them out.\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "\n",
    "    context: a list of strings\n",
    "    \n",
    "    quotes : a list of strings\n",
    "    \n",
    "    movies : a list of strings\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "\n",
    "    context, quotes, movies : a pruned context, movies, and quotes, based on quote pruning\n",
    "    \"\"\"\n",
    "    kill_sign = \"asdf no good this quote is toodleloos\"\n",
    "    single_list = (\".\", \"/\", \":\", \",\", \";\", \"!\", \"?\", \"(\", \")\", '\"', \"-\", \"]\", \"'\", \"[\", \"#\", \"$\", \n",
    "                  \"well\", \"what\", \"why\", \"goodnight\", \"hello\", \"how\", \"no\", \"yes\", \"yep\", \"nope\", \"pick\", \"cigarette\", \n",
    "                  \"obviously\", \"kay\", \"scrappy\", \"good\", \"bad\", \"great\", \"goodbye\", \"who\", \"hoke\", \"yes'm\", \"come\",\n",
    "                  \"ouch\", \"huh\", \"shit\", \"ed\", \"fuck\", \"oh\", \"right\", \"ebay\", \"nothing\", \"me\", \"you\", \"mount\", \"pray\", \"sometimes\",\n",
    "                  \"really\", \"ditto\", \"jeez\", \"exactly\", \"bull\", \"bullshit\", \"yep\", \"bing\", \"38\", \"occupation\", \"pyscho\", \"ok\", \n",
    "                  \"okay\", \"ooh\", \"dance\", \"terrific\", \"cuban\", \"mexican\", \"but\", \"blue\", \"wood\", \"apples\", \"cider\", \"exactly\",\n",
    "                  \"david\", \"fire\", \"y'all\", \"so\", \"always\", \"hey\")\n",
    "    \n",
    "    index_list = []\n",
    "    \n",
    "    for x in range(len(quotes)):\n",
    "        y = quotes[x].split()\n",
    "        if len(y) == 1:\n",
    "            no_punct = punct_strip(y[0])\n",
    "            if (no_punct in single_list) or (len(no_punct) == 0):\n",
    "                context[x] = kill_sign\n",
    "                quotes[x] = kill_sign\n",
    "                movies[x] = kill_sign\n",
    "        elif len(y) == 0:\n",
    "            context[x] = kill_sign\n",
    "            quotes[x] = kill_sign\n",
    "            movies[x] = kill_sign \n",
    "    \n",
    "    # Remove all kill signs\n",
    "    while (kill_sign in movies):\n",
    "        context.remove(kill_sign)\n",
    "        quotes.remove(kill_sign)\n",
    "        movies.remove(kill_sign)\n",
    "        \n",
    "    return context, quotes, movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_words(inv_ind):\n",
    "    \"\"\"\n",
    "    Returns the set of all non-stop-word in the inverted index\n",
    "    \"\"\"\n",
    "    # Get English stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # Find set of words that aren't stop words and are in our tfidf\n",
    "    base_words = []\n",
    "    word_to_index = {}\n",
    "    for word in inv_ind:\n",
    "        if word not in stop_words:\n",
    "            base_words.append(word)\n",
    "    for i, word in enumerate(base_words):\n",
    "        word_to_index[word] = i\n",
    "    \n",
    "    # Return word list and mapping\n",
    "    return base_words, word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_word_counts(lst, word):\n",
    "    \"\"\"Update the list for a word co-occurrance matrix\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "\n",
    "    lst: the pair (word,count) list from the word occurrance\n",
    "    \n",
    "    word : the word we're updating\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "\n",
    "    new_list : an updated word occurrance list\n",
    "    \"\"\"\n",
    "    found = False\n",
    "    new_list = []\n",
    "    for w,c in lst:\n",
    "        if word == w:\n",
    "            found = True\n",
    "            count = c+1\n",
    "            new_list.append((w,count))\n",
    "            break\n",
    "        else:\n",
    "            new_list.append((w,c))\n",
    "    if not(found):\n",
    "        new_list.append((word,1))\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_basic_cooccurance(word_list):\n",
    "    \"\"\" Initialize the base word co-occurrance (unigram) list from our context and quotes.\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "\n",
    "    word_list: the list of words which are in our movie space\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "\n",
    "    word_co : a dictionary representing the word_occurrance matrix\n",
    "    \"\"\"\n",
    "    # Get English stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # Merge context and quotes\n",
    "    quote_list = f_quotes\n",
    "    new_quote_list = []\n",
    "    for q in quote_list:\n",
    "        new_q = punct_strip(q)\n",
    "        if new_q not in f_context:\n",
    "            new_quote_list.append(new_q)\n",
    "    context_quotes = f_context + new_quote_list\n",
    "    \n",
    "    # Find co occurances in context data, based co-occurances in a document\n",
    "    word_co = defaultdict(list)\n",
    "    word_count_dict = defaultdict(int)\n",
    "    for doc in context_quotes:\n",
    "        # Double loop to count word co-occurances\n",
    "        tkns = f_tokenizer.tokenize(doc)\n",
    "        for i in range(len(tkns)):\n",
    "            if tkns[i] not in stop_words:\n",
    "                word_count_dict[tkns[i]] += 1\n",
    "                for j in range(len(tkns)):\n",
    "                    if not(j == i) and (tkns[j] in word_list):\n",
    "                        word_co[tkns[i]] = update_word_counts(word_co[tkns[i]], tkns[j])\n",
    "    return word_co, word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_cooccurance(word_co_old, word_count_dict_old, word_list, docs):\n",
    "    \"\"\" Updates the word co-occurrance mat and the word count dict with a new set of data.\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "\n",
    "    word_co: a word co-occurrance matrix in the form of a dictionary\n",
    "    \n",
    "    word_count_dict: a dictionary that keeps track of the total occurances of a word\n",
    "    \n",
    "    word_list: the list of words which are in our movie space\n",
    "    \n",
    "    docs: a list of new docs we're using to update our word co-occurance\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "    \n",
    "    word_co, word_count_dict : new word co-occurance dict/mat and new word count dictionary\n",
    "    \"\"\"\n",
    "    # Get English stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # Make init dict\n",
    "    word_co = defaultdict(list)\n",
    "    word_count_dict = defaultdict(int)\n",
    "    word_co.update(word_co_old)\n",
    "    word_count_dict.update(word_count_dict_old)\n",
    "    \n",
    "    # Find co occurances in context data, based on document (content)\n",
    "    for doc in docs:\n",
    "        # Double loop to count word co-occurances\n",
    "        tkns = f_tokenizer.tokenize(punct_strip(doc))\n",
    "        for i in range(len(tkns)):\n",
    "            if tkns[i] not in stop_words:\n",
    "                word_count_dict[tkns[i]] += 1\n",
    "                for j in range(len(tkns)):\n",
    "                    if not(j == i) and (tkns[j] in word_list):\n",
    "                        word_co[tkns[i]] = update_word_counts(word_co[tkns[i]], tkns[j])\n",
    "    return word_co, word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pmi(word_co, word_count_dict):\n",
    "    \"\"\" Calculate the pmi of a word based on the word co-occurances\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "\n",
    "    word_co: a word occurrance matrix in the form of a dictionary\n",
    "    \n",
    "    word_list: the list of words which are in our movie space\n",
    "    \n",
    "    docs: the new docs we're using to update our word co-occurance\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "    pmi_dict = a dictionary like word_co but has the pmi's instead\n",
    "    \"\"\"\n",
    "    # PMI(x,y) = log[p(x,y)/(p(x)*p(y))], assuming p(x,y) = co-occurances over total\n",
    "    pmi_dict = defaultdict(list)\n",
    "    \n",
    "    # Find total\n",
    "    total = 0\n",
    "    for key in word_count_dict:\n",
    "        total += word_count_dict[key]\n",
    "    \n",
    "    # Caclulate PMIs\n",
    "    for key in word_co:\n",
    "        p_x = (word_count_dict[key] + 0.0) / total\n",
    "        pmi_list = []\n",
    "        lst = word_co[key]\n",
    "        for word,co in lst:\n",
    "            p_y = (word_count_dict[word] + 0.0) / total\n",
    "            p_x_y = (co + 0.0) / total\n",
    "            res = math.log((p_x_y / (p_x * p_y)))\n",
    "            pmi_list.append((word,res))\n",
    "        pmi_list.sort(key=lambda tup:tup[1], reverse=True) # sort by second item, the pmi\n",
    "        pmi_dict[key] = pmi_list\n",
    "    return pmi_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------#\n",
    "#--------------------------Final Version Init------------------------------------#\n",
    "#--------------------------------------------------------------------------------#\n",
    "context_file = \"final_context_2.json\"\n",
    "movie_file = \"final_movies_2.json\"\n",
    "quote_file = \"final_quotes_2.json\"\n",
    "year_rating_file = \"final_year_rating_2.json\"\n",
    "\n",
    "f_context = read_file(context_file)\n",
    "f_movies = read_file(movie_file)\n",
    "f_quotes = read_file(quote_file)\n",
    "f_year_rating_dict = read_file(year_rating_file)\n",
    "\n",
    "\n",
    "# Reincode to unicode\n",
    "for i in range(len(f_context)):\n",
    "    f_context[i] = f_context[i].encode(\"utf-8\").decode(\"utf-8\")\n",
    "    f_movies[i] = f_movies[i].encode(\"utf-8\").decode(\"utf-8\")\n",
    "    f_quotes[i] = f_quotes[i].encode(\"utf-8\").decode(\"utf-8\")\n",
    "\n",
    "f_context, f_quotes, f_movies = quote_pruner(f_context, f_quotes, f_movies)\n",
    "    \n",
    "# Initialize query tokenizer\n",
    "f_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# Create inverted index mapping terms to quotes\n",
    "\"\"\"\n",
    "f_inverted_index = {}\n",
    "for context_id, context in enumerate(f_context):\n",
    "    context_tokens = f_tokenizer.tokenize(context)\n",
    "    for term in context_tokens:\n",
    "        if term not in stop_words:\n",
    "            if term in f_inverted_index:\n",
    "                lst = f_inverted_index[term]\n",
    "                found = False\n",
    "                for i, tup in enumerate(lst):\n",
    "                    if context_id == tup[0]:\n",
    "                        lst[i] = (context_id, tup[1] + 1)\n",
    "                        found = True\n",
    "                if not found:\n",
    "                    f_inverted_index[term].append((context_id, 1))\n",
    "            else:\n",
    "                f_inverted_index[term] = [(context_id, 1)]\n",
    "\"\"\"\n",
    "f_inverted_index = {} # Including Stop Words\n",
    "for context_id, context in enumerate(f_context):\n",
    "    context_tokens = f_tokenizer.tokenize(context)\n",
    "    for term in context_tokens:\n",
    "        if term in f_inverted_index:\n",
    "            lst = f_inverted_index[term]\n",
    "            found = False\n",
    "            for i, tup in enumerate(lst):\n",
    "                if context_id == tup[0]:\n",
    "                    lst[i] = (context_id, tup[1] + 1)\n",
    "                    found = True\n",
    "            if not found:\n",
    "                f_inverted_index[term].append((context_id, 1))\n",
    "        else:\n",
    "            f_inverted_index[term] = [(context_id, 1)]\n",
    "\n",
    "# Compute idf values for each term\n",
    "f_idf = compute_idf(f_inverted_index, len(f_context))\n",
    "# Prune out values removed by idf\n",
    "f_inverted_index = {key: val for key, val in f_inverted_index.items() if key in f_idf}\n",
    "# Compute document norms\n",
    "f_norms = compute_doc_norms(f_inverted_index, f_idf, len(f_context))\n",
    "\n",
    "# Check if there is a word co-occurrance file, word count file, and pmi file. if not, remake\n",
    "word_co_filename = \"word_co_2.json\"\n",
    "word_count_filename = \"word_count_dict_2.json\"\n",
    "pmi_dict_filename = \"pmi_dict_2.json\"\n",
    "if ((os.path.isfile(word_co_filename)) and (os.path.isfile(word_count_filename))) and (os.path.isfile(pmi_dict_filename)):\n",
    "    # Read files\n",
    "    word_co = read_file(word_co_filename)\n",
    "    word_count_dict = read_file(word_count_filename)\n",
    "    pmi_dict = read_file(pmi_dict_filename)\n",
    "else:\n",
    "    # Initialize word co-occurance matrix (merge this with above cell)\n",
    "    # This may take a while\n",
    "    word_list, word_to_index = get_context_words(f_inverted_index)\n",
    "    #word_co, word_count_dict = find_basic_cooccurance(word_list)\n",
    "    \n",
    "    # Update with more data\n",
    "    # Update word co-occurrence mat with new data\n",
    "    word_co = defaultdict(list)\n",
    "    word_count_dict = defaultdict(int)\n",
    "    word_co_file = \"imdb_quote_data_all_2.json\"\n",
    "    word_co_data = read_file(word_co_file)\n",
    "    word_co, word_count_dict = update_cooccurance(word_co, word_count_dict, word_list, word_co_data)\n",
    "    \n",
    "    # Get PMI\n",
    "    pmi_dict = find_pmi(word_co, word_count_dict)\n",
    "    # Write data\n",
    "    with io.open(\"word_co_2.json\",'w',encoding=\"utf-8\") as fout:\n",
    "        fout.write(unicode(json.dumps(word_co, ensure_ascii=False)))\n",
    "    with io.open(\"word_count_dict_2.json\",'w',encoding=\"utf-8\") as fout:\n",
    "        fout.write(unicode(json.dumps(word_count_dict, ensure_ascii=False)))\n",
    "    with io.open(\"pmi_dict.json_2\",'w',encoding=\"utf-8\") as fout:\n",
    "        fout.write(unicode(json.dumps(pmi_dict, ensure_ascii=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------NEW METHODS-----------------------------------#\n",
    "def year_rating_weight(year, rating, cosine, cur_year=2016, min_year=1925, year_weight=0.3,\n",
    "                      rating_weight=0.7, cosine_weight=0.8, y_r_weight=0.2):\n",
    "    \n",
    "    \"\"\" Compute new score with weighting from the cosine similarity with\n",
    "    the release year and rating of the movie.\n",
    "\n",
    "    Arguments\n",
    "    =========\n",
    "\n",
    "    year: float, the year of the movie\n",
    "\n",
    "    rating: float, the rating of the movie (out of 10)\n",
    "\n",
    "    cosine: float, the cosine similarity of the query against the movie context\n",
    "\n",
    "    cur_year, min_year: int, current year and minimum year (the lowest year)\n",
    "    \n",
    "    year_weight, rating_weight: the weights for the year and rating\n",
    "    \n",
    "    cosine_weight, y_r_weight: the weights for the cosine sim vs the year and rating value\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "\n",
    "    a new score that has been weighted with the year and rating of the movie \n",
    "    \"\"\"\n",
    "    \n",
    "    w_year = (((cur_year-min_year)-(cur_year-year))/(cur_year-min_year))*year_weight\n",
    "    w_rating = (rating/10.0)*rating_weight\n",
    "    return ((w_year+w_rating)*y_r_weight) + (cosine*cosine_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_vectorize(q, sw=False):\n",
    "    # Remove punctuation, lowercase, and encode to utf\n",
    "    query = punct_strip(q.lower().encode(\"utf-8\").decode(\"utf-8\"))\n",
    "    \n",
    "    # Tokenize query and check query stopword cutoff\n",
    "    query_words = f_tokenizer.tokenize(query)\n",
    "    \n",
    "    # Remove stop words if necessary\n",
    "    stop_words = stopwords.words('english') # Get English stop words\n",
    "    if (sw):\n",
    "        new_query = []\n",
    "        for x in query_words:\n",
    "            if x not in stop_words:\n",
    "                new_query.append(x)\n",
    "        query_words = new_query\n",
    "    \n",
    "    # Make query tfidf\n",
    "    query_tfidf = defaultdict(int)\n",
    "    for word in query_words:\n",
    "        query_tfidf[word] += 1\n",
    "    for word in query_tfidf:\n",
    "        if word in f_idf:\n",
    "            query_tfidf[word] *= f_idf[word]\n",
    "        else:\n",
    "            query_tfidf[word] = 0\n",
    "    \n",
    "    # Find query norm\n",
    "    query_norm = 0\n",
    "    for word in query_tfidf:\n",
    "        query_norm += math.pow(query_tfidf[word], 2)\n",
    "    query_norm = math.sqrt(query_norm)\n",
    "    \n",
    "    return query_tfidf, query_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_rocchio(query_tfidf, query_norm, relevant, sw=False, a=.3, b=.4, clip = True):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        query: a string representing the name of the movie being queried for\n",
    "        \n",
    "        relevant: a list of int representing the indices of relevant movies for query\n",
    "        \n",
    "        irrelevant: a list of strings representing the names of irrelevant movies for query\n",
    "        \n",
    "        a,b: floats, corresponding to the weighting of the original query, relevant queriesrespectively.\n",
    "        \n",
    "        clip: boolean, whether or not to clip all returned negative values to 0\n",
    "        \n",
    "    Returns:\n",
    "        q_mod: a dict representing the modified query vector. this vector should have no negatve\n",
    "        weights in it!\n",
    "    \"\"\"\n",
    "    \n",
    "    relevant_id = []\n",
    "    for s,i in relevant:\n",
    "        relevant_id.append(i)\n",
    "    \n",
    "    if query_norm == 0:\n",
    "        return f_find_random()\n",
    "    \n",
    "    # Calculate alpha*query_vec\n",
    "    query_vec = query_tfidf\n",
    "    for word in query_vec:\n",
    "        query_vec[word] /= query_norm\n",
    "        query_vec[word] *= a\n",
    "    \n",
    "    # Get words in relevant docs\n",
    "    relevant_words = []\n",
    "    relevant_context = []\n",
    "    for i in relevant_id:\n",
    "        relevant_context.append(f_context[i])\n",
    "    for context in relevant_context:\n",
    "        context_tkns = f_tokenizer.tokenize(context)\n",
    "        for tkn in context_tkns:\n",
    "            if tkn not in relevant_words:\n",
    "                relevant_words.append(tkn)\n",
    "    \n",
    "    # Collect relevant doc vector sums\n",
    "    relevant_docs = defaultdict(int)\n",
    "    for word in relevant_words:\n",
    "        if word in f_inverted_index:\n",
    "            for quote_id, tf in f_inverted_index[word]:\n",
    "                if quote_id in relevant_id:\n",
    "                    relevant_docs[word] += (tf / f_norms[quote_id])\n",
    "    \n",
    "    # Calculate beta term\n",
    "    beta_term = b * (1.0 / len(relevant))\n",
    "    for key in relevant_docs:\n",
    "        relevant_docs[key] *= beta_term\n",
    "    \n",
    "    # Sum query and relevant\n",
    "    q_mod = {k: query_vec.get(k,0) + relevant_docs.get(k,0.0) for k in set(query_vec) | set(relevant_docs)}\n",
    "\n",
    "    # negative checks for terms, if clip\n",
    "    if (clip):\n",
    "        for key in q_mod:\n",
    "            if q_mod[key] < 0:\n",
    "                q_mod[key] = 0\n",
    "        return q_mod\n",
    "    else:\n",
    "        return q_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_final(q, rocchio=True, psuedo_rocchio_num=5, sw=False, pmi_num=8):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        q: a string representing the query\n",
    "        \n",
    "        rocchio: a boolean representing whether or not to use psuedo relevance feedback with Rocchio\n",
    "        \n",
    "        psudo_rocchio_num: and int representing the number of top documents to consider relevant for rocchio\n",
    "        \n",
    "        sw: a boolean on whether or not to include stop words. \n",
    "        \n",
    "        pmi_num: an int representing the number of items to add to the query to expand it with PMI.\n",
    "        \n",
    "    Returns:\n",
    "        result_quotes: a list of the top x results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Vectorize query\n",
    "    query_tfidf, query_norm = query_vectorize(q, sw)\n",
    "    \n",
    "    if query_norm == 0:\n",
    "        return f_quotes(randint(0,len(f_quotes)))\n",
    "    \n",
    "    # Expand query using PMI\n",
    "    # http://www.jofcis.com/publishedpapers/2011_7_1_17_24.pdf\n",
    "    pmi_expansion = defaultdict(float)\n",
    "    pmi_norm = 1\n",
    "    for word in query_tfidf: # Sum PMI lists\n",
    "        if word in pmi_dict.keys():\n",
    "            pmi_list = pmi_dict[word][:pmi_num]\n",
    "            pmi_score_list = []\n",
    "            for word,score in pmi_list:\n",
    "                pmi_expansion[word] += score\n",
    "                pmi_score_list.append(score)\n",
    "            temp_norm = 0\n",
    "            for s in pmi_score_list:\n",
    "                temp_norm += math.pow(s, 2)\n",
    "            temp_norm = math.sqrt(query_norm)\n",
    "            pmi_norm *= temp_norm\n",
    "    query_tfidf.update(pmi_expansion)\n",
    "    query_norm = query_norm * 2 * pmi_num * pmi_norm\n",
    "    \n",
    "    # Find query norm\n",
    "    query_norm = 0\n",
    "    for word in query_tfidf:\n",
    "        query_norm += math.pow(query_tfidf[word], 2)\n",
    "    query_norm = math.sqrt(query_norm)\n",
    "    \n",
    "    # Get scores\n",
    "    scores = [0 for _ in f_quotes]\n",
    "    for word in query_tfidf:\n",
    "        if word in f_inverted_index:\n",
    "            for quote_id, tf in f_inverted_index[word]:\n",
    "                scores[quote_id] += query_tfidf[word]*tf*f_idf[word]\n",
    "\n",
    "    results = []\n",
    "    for i, s in enumerate(scores):\n",
    "        if f_norms[i] != 0:\n",
    "            results.append((s/(f_norms[i]*query_norm), i))\n",
    "    \n",
    "    # Weight scores with year and rating\n",
    "    for i in range(len(results)):\n",
    "        score = results[i][0]\n",
    "        index = results[i][1]\n",
    "        year = f_year_rating_dict[f_movies[i]][0]\n",
    "        rating = f_year_rating_dict[f_movies[i]][1]\n",
    "        results[i] = (year_rating_weight(float(year), float(rating), score), index)\n",
    "    \n",
    "    # sort results\n",
    "    results.sort(reverse=True)\n",
    "    \n",
    "    if rocchio:\n",
    "        # Do pseudo-relevance feedback with Rocchio\n",
    "        mod_query = pseudo_rocchio(query_tfidf, query_norm, results[:psuedo_rocchio_num], sw)\n",
    "        mod_query_norm = 0\n",
    "        for word in mod_query:\n",
    "            mod_query_norm += math.pow(mod_query[word], 2)\n",
    "        mod_query_norm = math.sqrt(mod_query_norm)\n",
    "\n",
    "\n",
    "        # Re-find scores and reweight with year and rating\n",
    "        scores = [0 for _ in f_quotes]\n",
    "        for word in mod_query:\n",
    "            if word in f_inverted_index:\n",
    "                for quote_id, tf in f_inverted_index[word]:\n",
    "                    scores[quote_id] += mod_query[word]*tf*f_idf[word]\n",
    "\n",
    "        results = []\n",
    "        for i, s in enumerate(scores):\n",
    "            if f_norms[i] != 0:\n",
    "                results.append((s/(f_norms[i]*mod_query_norm), i))\n",
    "\n",
    "        # Weight scores with year and rating\n",
    "        for i in range(len(results)):\n",
    "            score = results[i][0]\n",
    "            index = results[i][1]\n",
    "            year = f_year_rating_dict[f_movies[i]][0]\n",
    "            rating = f_year_rating_dict[f_movies[i]][1]\n",
    "            results[i] = (year_rating_weight(float(year), float(rating), score), index)\n",
    "    \n",
    "    # Sort and return results\n",
    "    top_res_num = 5\n",
    "    results.sort(reverse=True)\n",
    "    used_quotes = []\n",
    "    return_res = []\n",
    "    counter = 0\n",
    "    while len(return_res) <= 5: # Avoid duplicate quotes\n",
    "        score, i = results[counter]\n",
    "        if f_quotes[i] not in used_quotes:\n",
    "            used_quotes.append(f_quotes[i])\n",
    "            return_res.append((score,i))\n",
    "        else:\n",
    "            counter += 1\n",
    "            \n",
    "    result_quotes = [\"{} - {}\".format(f_quotes[i].encode('utf-8'), f_movies[i].encode('utf-8')) for _, i in return_res[:top_res_num]]\n",
    "    return result_quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_query(q, rocchio=True):\n",
    "    x = find_final(q, rocchio)\n",
    "    for y in x:\n",
    "        print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_context(q):\n",
    "    for x in range(len(f_context)):\n",
    "        if f_quotes[x] == q:\n",
    "            print(f_context[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probably. Am I a suspect? - Prisoners\n",
      "It's called sitting. - Sky High\n",
      "What did you see? - Orphan\n",
      "Sleeping. Why are you dressed? - Ocean's Twelve\n",
      "What are you doing? - Ocean's Twelve\n"
     ]
    }
   ],
   "source": [
    "system_query(\"what are you doing saturday\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's called sitting. - Sky High\n",
      "What did you see? - Orphan\n",
      "Sleeping. Why are you dressed? - Ocean's Twelve\n",
      "What are you doing? - Ocean's Twelve\n",
      "Probably. Am I a suspect? - Prisoners\n"
     ]
    }
   ],
   "source": [
    "system_query(\"what are you doing saturday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}